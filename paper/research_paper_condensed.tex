\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{graphicx}
\usepackage{float}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage{xcolor}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{cite}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[margin=1in]{geometry}
\usepackage{needspace}
\usepackage{hyperref}

% Section formatting
\usepackage{titlesec}
\newcommand{\sectionbreak}{\clearpage}
\titlespacing*{\section}{0pt}{12pt}{6pt}
\titlespacing*{\subsection}{0pt}{12pt}{6pt}

\title{\LARGE \bf
Real-Time Obstacle Avoidance Using \\
Uncertainty-Guided Adaptive Region Fusion \\
for Autonomous Navigation using Monocular Vision
}

\author{
\normalsize Md. Shakib Hossen\\
\normalsize Department of Computer Science and Engineering\\
\normalsize Begum Rokeya University, Rangpur\\
\normalsize \texttt{shakib.1905017@student.brur.ac.bd}
}

\begin{document}

\maketitle

\section{Discussion}

The experimental results demonstrate that uncertainty-guided adaptive fusion represents a significant advancement in monocular obstacle avoidance systems. Our approach achieves 7.4\% better navigation accuracy than YOLOv8-only baselines~\cite{jocher2023ultralytics} while reducing false safe rates by 3.4\%, validating the effectiveness of dynamic fusion based on depth estimation uncertainty~\cite{kendall2017uncertainties}. The system maintains real-time performance (24.5 FPS) with only 15\% computational overhead for uncertainty quantification~\cite{poggi2020uncertainty}.

\subsection{Key Achievements}

Our work demonstrates several significant improvements in monocular navigation:

\begin{itemize}
\item \textbf{Accuracy Improvement}: The uncertainty-guided fusion approach achieves 13.9\% improvement over depth-only methods, particularly in challenging indoor environments where traditional approaches struggle~\cite{ranftl2020towards}.

\item \textbf{Safety Enhancement}: Critical false safe rates reduced to 4.8\%, representing a 41\% improvement over fixed fusion methods, crucial for practical deployment in autonomous systems.

\item \textbf{Real-Time Performance}: Achieved 24.5 FPS on consumer hardware (MacBook Air M1) and 11.2 FPS on edge devices (Jetson TX2), demonstrating scalability across platforms.

\item \textbf{Computational Efficiency}: Monte Carlo dropout implementation adds only 15\% overhead while providing robust uncertainty estimates for fusion guidance.
\end{itemize}

\subsection{Limitations and Future Work}

Several limitations require acknowledgment:

\begin{itemize}
\item \textbf{Depth Ambiguity}: MiDaS produces relative rather than metric depth, requiring careful calibration for absolute distance estimation.

\item \textbf{Environmental Sensitivity}: Performance degrades in extreme lighting conditions and with transparent obstacles, common challenges in real-world deployment.

\item \textbf{Processing Overhead}: While achieving real-time performance, the system requires careful optimization for resource-constrained platforms.

\item \textbf{Dynamic Scenes}: Current approach has limited effectiveness with fast-moving objects, necessitating future work on temporal integration.
\end{itemize}

Future research directions include:

\begin{itemize}
\item \textbf{Multi-Modal Integration}: Incorporating IMU data for scale recovery and enhanced motion handling.

\item \textbf{Edge Optimization}: Further performance improvements for embedded systems through model compression and quantization.

\item \textbf{Temporal Analysis}: Implementation of multi-frame tracking for improved stability and dynamic object handling.

\item \textbf{Adaptive Parameters}: Development of environment-specific parameter tuning for optimal performance across conditions.
\end{itemize}

\subsection{Broader Impact}

This work has significant implications for autonomous robotics:

\begin{itemize}
\item \textbf{Cost Reduction}: Monocular approach enables wider adoption compared to expensive sensor suites.

\item \textbf{Safety Improvements}: Uncertainty-aware decision making enhances reliability in safety-critical applications.

\item \textbf{Deployment Flexibility}: Demonstrated performance across consumer and edge computing platforms enables diverse applications.

\item \textbf{Research Advancement}: Novel uncertainty quantification and fusion strategies contribute to the broader field of robust autonomy.
\end{itemize}

This research represents a significant step toward reliable, cost-effective autonomous navigation systems, with particular relevance for indoor robotics, autonomous vehicles, and edge computing applications.

\section*{Acknowledgments}

The authors acknowledge the valuable computational resources provided by the university computing infrastructure and the open-source community for providing the foundational models and frameworks that enabled this research.

\begin{thebibliography}{99}

\bibitem{jocher2023ultralytics}
G. Jocher, A. Chaurasia, and J. Qiu,
``YOLO by Ultralytics,''
\emph{https://github.com/ultralytics/ultralytics}, 2023.

\bibitem{kendall2017uncertainties}
A. Kendall and Y. Gal,
``What uncertainties do we need in bayesian deep learning for computer vision?''
\emph{Advances in Neural Information Processing Systems}, vol. 30, 2017.

\bibitem{poggi2020uncertainty}
M. Poggi, F. Aleotti, F. Tosi, and S. Mattoccia,
``On the uncertainty of self-supervised monocular depth estimation,''
\emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp. 3227--3237, 2020.

\bibitem{ranftl2020towards}
R. Ranftl, K. Lasinger, D. Hafner, K. Schindler, and V. Koltun,
``Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer,''
\emph{IEEE Transactions on Pattern Analysis and Machine Intelligence}, vol. 44, no. 3, pp. 1623--1637, 2020.

\end{thebibliography}

\end{document}
